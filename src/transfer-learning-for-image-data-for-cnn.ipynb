{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, time, copy, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"applied-ml-assignment-5/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHICKEN_IMAGES_PATH = \"../data/chicken/data\"\n",
    "DUCK_IMAGES_PATH = \"../data/duck/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotations_file(paths: list[str]):\n",
    "    # training dataset\n",
    "    train = pd.DataFrame(columns=[\"path\", \"class\"])\n",
    "    for path in paths:\n",
    "        full_path = os.path.join(path, \"train\")\n",
    "        image_names = [f\"{path}/train/{_}\" for _ in os.listdir(full_path)[:100]]\n",
    "        label = path.split(\"/\")[2]\n",
    "        temp = pd.DataFrame({\"path\": image_names, \"class\": label})\n",
    "        train = pd.concat([train, temp])\n",
    "\n",
    "    # validation dataset\n",
    "    val = pd.DataFrame(columns=[\"path\", \"class\"])\n",
    "    for path in paths:\n",
    "        full_path = os.path.join(path, \"val\")\n",
    "        image_names = [f\"{path}/val/{_}\" for _ in os.listdir(full_path)[:50]]\n",
    "        label = path.split(\"/\")[2]\n",
    "        temp = pd.DataFrame({\"path\": image_names, \"class\": label})\n",
    "        val = pd.concat([val, temp])\n",
    "\n",
    "    # test dataset\n",
    "    test = pd.DataFrame(columns=[\"path\", \"class\"])\n",
    "    for path in paths:\n",
    "        full_path = os.path.join(path, \"test\")\n",
    "        image_names = [f\"{path}/test/{_}\" for _ in os.listdir(full_path)[:100]]\n",
    "        label = path.split(\"/\")[2]\n",
    "        temp = pd.DataFrame({\"path\": image_names, \"class\": label})\n",
    "        test = pd.concat([test, temp])\n",
    "\n",
    "    train = train.reset_index(drop=True)\n",
    "    val = val.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "train, val, test = create_annotations_file([CHICKEN_IMAGES_PATH, DUCK_IMAGES_PATH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChickenOrDuck(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, annotations_file, transforms=transforms.Compose([transforms.ToTensor()])\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.annotations_file = annotations_file\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.annotations_file.loc[index, \"path\"]\n",
    "        image_class = self.annotations_file.loc[index, \"class\"]\n",
    "        image_label = 1 if image_class == \"chicken\" else 0\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize((128, 128))\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        return image, image_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "image_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "train_dataset = ChickenOrDuck(train, image_transforms[\"train\"])\n",
    "val_dataset = ChickenOrDuck(val, image_transforms[\"val\"])\n",
    "test_dataset = ChickenOrDuck(test, image_transforms[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 2e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [32, 2]                   --\n",
       "├─Conv2d: 1-1                            [32, 64, 64, 64]          9,408\n",
       "├─BatchNorm2d: 1-2                       [32, 64, 64, 64]          128\n",
       "├─ReLU: 1-3                              [32, 64, 64, 64]          --\n",
       "├─MaxPool2d: 1-4                         [32, 64, 32, 32]          --\n",
       "├─Sequential: 1-5                        [32, 64, 32, 32]          --\n",
       "│    └─BasicBlock: 2-1                   [32, 64, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-1                  [32, 64, 32, 32]          36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [32, 64, 32, 32]          128\n",
       "│    │    └─ReLU: 3-3                    [32, 64, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-4                  [32, 64, 32, 32]          36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [32, 64, 32, 32]          128\n",
       "│    │    └─ReLU: 3-6                    [32, 64, 32, 32]          --\n",
       "│    └─BasicBlock: 2-2                   [32, 64, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-7                  [32, 64, 32, 32]          36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [32, 64, 32, 32]          128\n",
       "│    │    └─ReLU: 3-9                    [32, 64, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-10                 [32, 64, 32, 32]          36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [32, 64, 32, 32]          128\n",
       "│    │    └─ReLU: 3-12                   [32, 64, 32, 32]          --\n",
       "├─Sequential: 1-6                        [32, 128, 16, 16]         --\n",
       "│    └─BasicBlock: 2-3                   [32, 128, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-13                 [32, 128, 16, 16]         73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [32, 128, 16, 16]         256\n",
       "│    │    └─ReLU: 3-15                   [32, 128, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-16                 [32, 128, 16, 16]         147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [32, 128, 16, 16]         256\n",
       "│    │    └─Sequential: 3-18             [32, 128, 16, 16]         8,448\n",
       "│    │    └─ReLU: 3-19                   [32, 128, 16, 16]         --\n",
       "│    └─BasicBlock: 2-4                   [32, 128, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-20                 [32, 128, 16, 16]         147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [32, 128, 16, 16]         256\n",
       "│    │    └─ReLU: 3-22                   [32, 128, 16, 16]         --\n",
       "│    │    └─Conv2d: 3-23                 [32, 128, 16, 16]         147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [32, 128, 16, 16]         256\n",
       "│    │    └─ReLU: 3-25                   [32, 128, 16, 16]         --\n",
       "├─Sequential: 1-7                        [32, 256, 8, 8]           --\n",
       "│    └─BasicBlock: 2-5                   [32, 256, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-26                 [32, 256, 8, 8]           294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [32, 256, 8, 8]           512\n",
       "│    │    └─ReLU: 3-28                   [32, 256, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-29                 [32, 256, 8, 8]           589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [32, 256, 8, 8]           512\n",
       "│    │    └─Sequential: 3-31             [32, 256, 8, 8]           33,280\n",
       "│    │    └─ReLU: 3-32                   [32, 256, 8, 8]           --\n",
       "│    └─BasicBlock: 2-6                   [32, 256, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-33                 [32, 256, 8, 8]           589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [32, 256, 8, 8]           512\n",
       "│    │    └─ReLU: 3-35                   [32, 256, 8, 8]           --\n",
       "│    │    └─Conv2d: 3-36                 [32, 256, 8, 8]           589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [32, 256, 8, 8]           512\n",
       "│    │    └─ReLU: 3-38                   [32, 256, 8, 8]           --\n",
       "├─Sequential: 1-8                        [32, 512, 4, 4]           --\n",
       "│    └─BasicBlock: 2-7                   [32, 512, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-39                 [32, 512, 4, 4]           1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [32, 512, 4, 4]           1,024\n",
       "│    │    └─ReLU: 3-41                   [32, 512, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-42                 [32, 512, 4, 4]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [32, 512, 4, 4]           1,024\n",
       "│    │    └─Sequential: 3-44             [32, 512, 4, 4]           132,096\n",
       "│    │    └─ReLU: 3-45                   [32, 512, 4, 4]           --\n",
       "│    └─BasicBlock: 2-8                   [32, 512, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-46                 [32, 512, 4, 4]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [32, 512, 4, 4]           1,024\n",
       "│    │    └─ReLU: 3-48                   [32, 512, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-49                 [32, 512, 4, 4]           2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [32, 512, 4, 4]           1,024\n",
       "│    │    └─ReLU: 3-51                   [32, 512, 4, 4]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [32, 512, 1, 1]           --\n",
       "├─Linear: 1-10                           [32, 2]                   1,026\n",
       "==========================================================================================\n",
       "Total params: 11,177,538\n",
       "Trainable params: 11,177,538\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 18.95\n",
       "==========================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 415.24\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 466.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_in = model.fc.in_features\n",
    "model.fc = nn.Linear(num_in, 2)\n",
    "summary(model, (32, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetTransferLearning(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = resnet18(ResNet18_Weights.IMAGENET1K_V1)\n",
    "        num_in = backbone.fc.in_features\n",
    "\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.feature_extractor.eval()\n",
    "\n",
    "        num_target_classes = 2\n",
    "        self.classifier = nn.Linear(num_in, num_target_classes)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     with torch.no_grad():\n",
    "    #         representations = self.feature_extractor(x).flatten(1)\n",
    "    #     x = self.classifier(representations)\n",
    "    #     return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx, datalaoder_idx=None):\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.classifier(representations)\n",
    "        loss = F.cross_entropy(x, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            params=filter(lambda p: p.requires_grad, self.classifier.parameters()),\n",
    "            lr=LR,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | feature_extractor | Sequential | 11.2 M\n",
      "1 | classifier        | Linear     | 1.0 K \n",
      "-------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18968e18cf18488eba88265a09bbeba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "model = ImagenetTransferLearning()\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    ")\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
